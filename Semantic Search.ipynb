{"cells":[{"cell_type":"markdown","metadata":{"id":"zzZbP0LM6m5z"},"source":["# Semantic Search: Sentence Transformers and Elasticsearch\n"]},{"cell_type":"markdown","metadata":{},"source":["### Table of Contents\n","\n","* [I. Requirements](#I)\n","    * [I.1. Installations](#I1)\n","    * [I.2 Packages](#I2)\n","    * [I.3 Datasets](#I3)\n","    \n","* [II. Sentence Transformers](#II)\n","    * [II.1 Model](#II1)\n","    * [II.2 Search with Transformer](#II2)\n","\n","* [III. Elasticsearch](#III)\n","    * [III.1 Insert data](#III1)\n","    * [III.2 Search with Elastic](#III2)\n"]},{"cell_type":"markdown","metadata":{},"source":["### I. Requirements <a id=\"I\"></a>"]},{"cell_type":"markdown","metadata":{},"source":["#### I.1. Installations <a id=\"I1\"></a>"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: sentence-transformers in c:\\users\\balla\\anaconda3\\lib\\site-packages (2.1.0)\n","Requirement already satisfied: nltk in c:\\users\\balla\\anaconda3\\lib\\site-packages (from sentence-transformers) (3.6.1)\n","Requirement already satisfied: scipy in c:\\users\\balla\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.6.2)\n","Requirement already satisfied: torchvision in c:\\users\\balla\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.12.0.dev20211223)\n","Requirement already satisfied: scikit-learn in c:\\users\\balla\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.24.1)\n","Requirement already satisfied: tokenizers>=0.10.3 in c:\\users\\balla\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.10.3)\n","Requirement already satisfied: tqdm in c:\\users\\balla\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.59.0)\n","Requirement already satisfied: numpy in c:\\users\\balla\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.20.1)\n","Requirement already satisfied: huggingface-hub in c:\\users\\balla\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.1.2)\n","Requirement already satisfied: torch>=1.6.0 in c:\\users\\balla\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.11.0.dev20211223)\n","Requirement already satisfied: transformers<5.0.0,>=4.6.0 in c:\\users\\balla\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.12.3)\n","Requirement already satisfied: sentencepiece in c:\\users\\balla\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.1.96)\n","Requirement already satisfied: typing_extensions in c:\\users\\balla\\anaconda3\\lib\\site-packages (from torch>=1.6.0->sentence-transformers) (3.7.4.3)\n","Requirement already satisfied: pyyaml>=5.1 in c:\\users\\balla\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (5.4.1)\n","Requirement already satisfied: packaging>=20.0 in c:\\users\\balla\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (20.9)\n","Requirement already satisfied: regex!=2019.12.17 in c:\\users\\balla\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.4.4)\n","Requirement already satisfied: sacremoses in c:\\users\\balla\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.0.46)\n","Requirement already satisfied: filelock in c:\\users\\balla\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (3.0.12)\n","Requirement already satisfied: requests in c:\\users\\balla\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2.26.0)\n","Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\balla\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.4.7)\n","Requirement already satisfied: joblib in c:\\users\\balla\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers) (1.0.1)\n","Requirement already satisfied: click in c:\\users\\balla\\anaconda3\\lib\\site-packages (from nltk->sentence-transformers) (7.1.2)\n","Requirement already satisfied: idna<4,>=2.5 in c:\\users\\balla\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (3.3)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\balla\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.26.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\balla\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\balla\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.6.0->sentence-transformers) (2021.10.8)\n","Requirement already satisfied: six in c:\\users\\balla\\anaconda3\\lib\\site-packages (from sacremoses->transformers<5.0.0,>=4.6.0->sentence-transformers) (1.16.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\balla\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (2.1.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\balla\\anaconda3\\lib\\site-packages (from torchvision->sentence-transformers) (8.2.0)\n","Requirement already satisfied: elasticsearch in c:\\users\\balla\\anaconda3\\lib\\site-packages (7.15.2)\n","Requirement already satisfied: urllib3<2,>=1.21.1 in c:\\users\\balla\\anaconda3\\lib\\site-packages (from elasticsearch) (1.26.7)\n","Requirement already satisfied: certifi in c:\\users\\balla\\anaconda3\\lib\\site-packages (from elasticsearch) (2021.10.8)\n"]}],"source":["# Install sentence-transformers\n","!pip install sentence-transformers\n","\n","# Install elasticsearch \n","!pip install elasticsearch"]},{"cell_type":"markdown","metadata":{},"source":["#### I.2. Packages <a id=\"I2\"></a>"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["import json\n","import os\n","\n","from IPython.display import display, HTML\n","\n","from sentence_transformers import SentenceTransformer, util\n","\n","from elasticsearch import Elasticsearch\n","from elasticsearch import helpers"]},{"cell_type":"markdown","metadata":{},"source":["#### I.3. Datasets <a id=\"I3\"></a>\n","\n","As corpus, we use all EMNLP publications from 2016 - 2018\n"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[],"source":["def get_papers_dataset(ulr_dataset=\"https://sbert.net/datasets/emnlp2016-2018.json\", dataset=\"data/emnlp2016-2018.json\"):\n","\n","    if not os.path.exists(dataset):\n","        util.http_get(ulr_dataset, dataset)\n","\n","    with open(dataset) as f:\n","        papers = json.load(f)\n","        return papers"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["{\n","    \"title\": \"Rule Extraction for Tree-to-Tree Transducers by Cost Minimization\",\n","    \"abstract\": \"Finite-state transducers give efficient representations of many Natural Language phenomena. They allow to account for complex lexicon restrictions encountered, without involving the use of a large set of complex rules difficult to analyze. We here show that these representations can be made very compact, indicate how to perform the corresponding minimization, and point out interesting linguistic side-effects of this operation.\",\n","    \"url\": \"http://aclweb.org/anthology/D16-1002\",\n","    \"venue\": \"EMNLP\",\n","    \"year\": \"2016\"\n","}\n"]}],"source":["papers = get_papers_dataset()\n","print(json.dumps(papers[0], indent=4))"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["#To encode the papers, we must combine the title and the abstracts to a single string\n","papers_texts = [paper['title'] + '[SEP]' + paper['abstract'] for paper in papers]"]},{"cell_type":"markdown","metadata":{},"source":["### II. Sentence Transformers <a id=\"II\"></a>\n","\n","Sentence-BERT(SBERT) is a modification of the BERT(Bidirectional Encoder Representations from\n","Transformers) capable of detecting semantic similarities between sentences. Thus, unlike traditional search that only finds documents based on matching, the SBERT approach can also find synonyms (semantic relations). \n","<center><img src=\"image/sbert.png\" width=\"800\" height=\"400\"/></center>"]},{"cell_type":"markdown","metadata":{},"source":["Ressources:\n","\n","- [Documentation Sentence transformers](https://pypi.org/project/sentence-transformers/)\n","- [SBERT models](https://www.sbert.net/docs/pretrained_models.html#model-overview)\n","- [Arxiv SBERT](https://arxiv.org/pdf/1908.10084.pdf)\n","- [Arxiv BERT](https://arxiv.org/pdf/1810.04805.pdf)"]},{"cell_type":"markdown","metadata":{},"source":["#### II.1. Model <a id=\"II1\"></a>"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":23214,"status":"ok","timestamp":1637251053871,"user":{"displayName":"Alhousseynou Ball","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08803678771430707022"},"user_tz":-60},"id":"Zi1rHmmjyv1B"},"outputs":[],"source":["#Load model SentenceTransformers\n","model = SentenceTransformer('all-MiniLM-L6-v2')\n","\n","#Compute embeddings for all articles\n","corpus_embeddings = model.encode(papers_texts, convert_to_tensor=True)"]},{"cell_type":"markdown","metadata":{},"source":["#### II.2. Search with Transformer <a id=\"II2\"></a>"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":212,"status":"ok","timestamp":1637251080295,"user":{"displayName":"Alhousseynou Ball","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08803678771430707022"},"user_tz":-60},"id":"gERA8S-00jbb"},"outputs":[],"source":["def search_papers(title, search_hits, papers=papers):\n","    html = \"\"\"\n","    <style type='text/css'>\n","    @import url('https://fonts.googleapis.com/css?family=Oswald&display=swap');\n","    table {\n","      border-collapse: collapse;\n","      width: 800px;\n","    }\n","    th, td {\n","        border: 1px solid #9e9e9e;\n","        padding: 5px;\n","        font: 13px Oswald;\n","    }\n","    </style>\n","    \"\"\"\n","\n","    html += \"<h3></h3><table><thead><tr><th>Score</th><th>Title</th><th>Venu</th><th>Year</th></tr></thead>\"\n","    for hit in search_hits:\n","        related_papers = papers[hit['corpus_id']]\n","        html += \"<tr><td>%.4f</td><td>%s</td><td>%s</td><td>%s</td></tr>\" % (hit['score'], related_papers['title'], related_papers['venue'], related_papers['year'])\n","    html += \"</table>\"\n","    \n","    print(\"Paper:\", title)\n","    \n","    display(HTML(html))"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":844},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1637251082676,"user":{"displayName":"Alhousseynou Ball","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08803678771430707022"},"user_tz":-60},"id":"4ehHgh_gYK_-","outputId":"55a88b58-5f2b-4699-c180-1318dfe538bf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Paper: Digital Voicing of Silent Speech\n"]},{"data":{"text/html":["\n","    <style type='text/css'>\n","    @import url('https://fonts.googleapis.com/css?family=Oswald&display=swap');\n","    table {\n","      border-collapse: collapse;\n","      width: 800px;\n","    }\n","    th, td {\n","        border: 1px solid #9e9e9e;\n","        padding: 5px;\n","        font: 13px Oswald;\n","    }\n","    </style>\n","    <h3></h3><table><thead><tr><th>Score</th><th>Title</th><th>Venu</th><th>Year</th></tr></thead><tr><td>0.4329</td><td>Nonparametric Bayesian Models for Spoken Language Understanding</td><td>EMNLP</td><td>2016</td></tr><tr><td>0.4313</td><td>Speech segmentation with a neural encoder model of working memory</td><td>EMNLP</td><td>2017</td></tr><tr><td>0.4186</td><td>Using Context Information for Dialog Act Classification in DNN Framework</td><td>EMNLP</td><td>2017</td></tr><tr><td>0.4144</td><td>Session-level Language Modeling for Conversational Speech</td><td>EMNLP</td><td>2018</td></tr><tr><td>0.4136</td><td>Charmanteau: Character Embedding Models For Portmanteau Creation</td><td>EMNLP</td><td>2017</td></tr><tr><td>0.3960</td><td>ICON: Interactive Conversational Memory Network for Multimodal Emotion Detection</td><td>EMNLP</td><td>2018</td></tr><tr><td>0.3958</td><td>A Co-Attention Neural Network Model for Emotion Cause Analysis with Emotional Context Awareness</td><td>EMNLP</td><td>2018</td></tr><tr><td>0.3893</td><td>Learning a Lexicon and Translation Model from Phoneme Lattices</td><td>EMNLP</td><td>2016</td></tr><tr><td>0.3808</td><td>Reasoning about Pragmatics with Neural Listeners and Speakers</td><td>EMNLP</td><td>2016</td></tr><tr><td>0.3786</td><td>Supervised Domain Enablement Attention for Personalized Domain Classification</td><td>EMNLP</td><td>2018</td></tr></table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["title='Digital Voicing of Silent Speech'\n","abstract='In this paper, we consider the task of digitally voicing silent speech, where silently mouthed words are converted to audible speech based on electromyography (EMG) sensor measurements that capture muscle impulses. While prior work has focused on training speech synthesis models from EMG collected during vocalized speech, we are the first to train from EMG collected during silently articulated speech. We introduce a method of training on silent EMG by transferring audio targets from vocalized to silent signals. Our method greatly improves intelligibility of audio generated from silent EMG compared to a baseline that only trains with vocalized data, decreasing transcription word error rate from 64% to 4% in one data condition and 88% to 68% in another. To spur further development on this task, we share our new dataset of silent and vocalized facial EMG measurements.'\n","\n","paper_embedding = model.encode(title+'[SEP]'+ abstract, convert_to_tensor=True)\n","search_hits = util.semantic_search(paper_embedding, corpus_embeddings, top_k=10)\n","\n","search_papers(title, search_hits[0])"]},{"cell_type":"markdown","metadata":{"id":"2vvUh1oHKrly"},"source":["### III. Elasticsearch <a id=\"III\"></a>\n","\n","Elasticsearch is a distributed, RESTful search and analytics engine capable of addressing a growing number of use cases[[elastic.co](https://www.elastic.co/elasticsearch/)]. ElasticSearch give the possibility to index dense vectors and use them for document scoring. \n","\n","An advantage of ElasticSearch is that it is easy to add new documents to an index and that we can store also other data along with our vectors. A disadvantage is the slow performance, as it compares the query embeddings with all stored embeddings. This has a linear run-time and might be too slow for large (>100k) corpora[[elasticsearch semantic search](https://github.com/UKPLab/sentence-transformers/tree/master/examples/applications/semantic-search#elasticsearch)].\n"]},{"cell_type":"markdown","metadata":{},"source":["Steps for running this part:\n","- [Download elasticsearch](https://www.elastic.co/fr/downloads/elasticsearch)\n","- On your terminal: Go to the folder elasticsearch and run ``/bin/elasticsearch``"]},{"cell_type":"markdown","metadata":{},"source":["Ressources:\n","\n","- [Elasticsearch](https://pypi.org/project/elasticsearch/)"]},{"cell_type":"markdown","metadata":{},"source":["#### III.1. Insert data <a id=\"III1\"></a>"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"data":{"text/plain":["{'name': 'LAPTOP-GV57OKPL',\n"," 'cluster_name': 'elasticsearch',\n"," 'cluster_uuid': '3Jcz9bwdTYCp20csRb0JTQ',\n"," 'version': {'number': '7.9.0',\n","  'build_flavor': 'default',\n","  'build_type': 'zip',\n","  'build_hash': 'a479a2a7fce0389512d6a9361301708b92dff667',\n","  'build_date': '2020-08-11T21:36:48.204330Z',\n","  'build_snapshot': False,\n","  'lucene_version': '8.6.0',\n","  'minimum_wire_compatibility_version': '6.8.0',\n","  'minimum_index_compatibility_version': '6.0.0-beta1'},\n"," 'tagline': 'You Know, for Search'}"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["es = Elasticsearch()\n","\n","es.info()"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"data":{"text/plain":["{'acknowledged': True, 'shards_acknowledged': True, 'index': 'papers'}"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["mappings = {\n","        \"properties\": {\n","        \"paper\": {\n","            \"type\": \"text\"\n","        },\n","        \"paper_vector\": {\n","            \"type\": \"dense_vector\",\n","            \"dims\": 384\n","        }\n","    }\n","}\n","\n","es.indices.create(index='papers', mappings=mappings, ignore=[400])"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Total papers inserted: 974\n"]}],"source":["rows = 0\n","requests = []\n","\n","for _id, (paper, embedding) in enumerate(zip(papers, corpus_embeddings)):\n","    requests.append({\"_op_type\": \"index\",\n","                \"_index\": \"papers\",\n","                \"_id\": _id,\n","                \"_source\": {\n","                    \"paper\": paper[\"title\"],\n","                    \"venue\": paper[\"venue\"],\n","                    \"year\": paper[\"year\"],\n","                    \"paper_vector\": embedding.numpy()\n","                    }\n","                })\n","    rows += 1\n","helpers.bulk(es, requests)\n","\n","print(\"Total papers inserted: {}\".format(rows))"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["def get_papers_elastic(query_text, size=10):\n","    \n","  query_embedding = model.encode(query_text)  \n","  query={\n","    \"script_score\": {\n","        \"query\": {\n","        \"match_all\": {}\n","        },\n","        \"script\": {\n","        \"source\": \"cosineSimilarity(params.queryVector, 'paper_vector') + 1.0\",\n","        \"params\": {\n","            \"queryVector\": query_embedding\n","        }\n","        }\n","    }\n","}\n","\n","  results = es.search(index=\"papers\", query=query, size=size)[\"hits\"][\"hits\"]\n","  results = [{\"score\": (result[\"_score\"] - 1.0), \"paper\": result[\"_source\"][\"paper\"],  \"venue\": result[\"_source\"][\"venue\"], \"year\": result[\"_source\"][\"year\"]} for result in results]\n","\n","  return results"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":215,"status":"ok","timestamp":1637245363129,"user":{"displayName":"Alhousseynou Ball","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08803678771430707022"},"user_tz":-60},"id":"XPOfhBJs0ZGs"},"outputs":[],"source":["def display_papers(query, rows):\n","    html = \"\"\"\n","    <style type='text/css'>\n","    @import url('https://fonts.googleapis.com/css?family=Oswald&display=swap');\n","    table {\n","      border-collapse: collapse;\n","      width: 800px;\n","    }\n","    th, td {\n","        border: 1px solid #9e9e9e;\n","        padding: 5px;\n","        font: 13px Oswald;\n","    }\n","    </style>\n","    \"\"\"\n","\n","    html += \"<h3></h3><table><thead><tr><th>Score</th><th>Title</th><th>Venu</th><th>Year</th></tr></thead>\"\n","    for result in rows:\n","        html += \"<tr><td>%.4f</td><td>%s</td><td>%s</td><td>%s</td></tr>\" % (result[\"score\"], result[\"paper\"], result[\"venue\"], result[\"year\"])\n","    html += \"</table>\"\n","    \n","    print(\"Paper:\", query)\n","    \n","    display(HTML(html))"]},{"cell_type":"markdown","metadata":{},"source":["#### III.2. Search with Elastic <a id=\"III2\"></a>"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":806},"executionInfo":{"elapsed":257,"status":"ok","timestamp":1637245364315,"user":{"displayName":"Alhousseynou Ball","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"08803678771430707022"},"user_tz":-60},"id":"6R55hXq7NEAO","outputId":"246cbff1-3714-48a9-88e1-24d803ea68cb"},"outputs":[{"name":"stdout","output_type":"stream","text":["Paper: Digital Voicing of Silent Speech\n"]},{"data":{"text/html":["\n","    <style type='text/css'>\n","    @import url('https://fonts.googleapis.com/css?family=Oswald&display=swap');\n","    table {\n","      border-collapse: collapse;\n","      width: 800px;\n","    }\n","    th, td {\n","        border: 1px solid #9e9e9e;\n","        padding: 5px;\n","        font: 13px Oswald;\n","    }\n","    </style>\n","    <h3></h3><table><thead><tr><th>Score</th><th>Title</th><th>Venu</th><th>Year</th></tr></thead><tr><td>0.4329</td><td>Nonparametric Bayesian Models for Spoken Language Understanding</td><td>EMNLP</td><td>2016</td></tr><tr><td>0.4313</td><td>Speech segmentation with a neural encoder model of working memory</td><td>EMNLP</td><td>2017</td></tr><tr><td>0.4186</td><td>Using Context Information for Dialog Act Classification in DNN Framework</td><td>EMNLP</td><td>2017</td></tr><tr><td>0.4144</td><td>Session-level Language Modeling for Conversational Speech</td><td>EMNLP</td><td>2018</td></tr><tr><td>0.4136</td><td>Charmanteau: Character Embedding Models For Portmanteau Creation</td><td>EMNLP</td><td>2017</td></tr><tr><td>0.3960</td><td>ICON: Interactive Conversational Memory Network for Multimodal Emotion Detection</td><td>EMNLP</td><td>2018</td></tr><tr><td>0.3893</td><td>Learning a Lexicon and Translation Model from Phoneme Lattices</td><td>EMNLP</td><td>2016</td></tr><tr><td>0.3808</td><td>Reasoning about Pragmatics with Neural Listeners and Speakers</td><td>EMNLP</td><td>2016</td></tr><tr><td>0.3786</td><td>Supervised Domain Enablement Attention for Personalized Domain Classification</td><td>EMNLP</td><td>2018</td></tr><tr><td>0.3718</td><td>Estimating Marginal Probabilities of n-grams for Recurrent Neural Language Models</td><td>EMNLP</td><td>2018</td></tr></table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"}],"source":["title='Digital Voicing of Silent Speech'\n","abstract='In this paper, we consider the task of digitally voicing silent speech, where silently mouthed words are converted to audible speech based on electromyography (EMG) sensor measurements that capture muscle impulses. While prior work has focused on training speech synthesis models from EMG collected during vocalized speech, we are the first to train from EMG collected during silently articulated speech. We introduce a method of training on silent EMG by transferring audio targets from vocalized to silent signals. Our method greatly improves intelligibility of audio generated from silent EMG compared to a baseline that only trains with vocalized data, decreasing transcription word error rate from 64% to 4% in one data condition and 88% to 68% in another. To spur further development on this task, we share our new dataset of silent and vocalized facial EMG measurements.'\n","rows = get_papers_elastic(title+'[SEP]'+ abstract)\n","display_papers(title, rows)"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"SentenceTransformer_Elmo_Elasticsearch.ipynb","provenance":[]},"interpreter":{"hash":"a5ef8f27d904a5b557a36629352eb894f168a2cf445c87256c5afda463bff78f"},"kernelspec":{"display_name":"Python 3.8.8 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"}},"nbformat":4,"nbformat_minor":0}
